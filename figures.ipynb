{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c4b50c",
   "metadata": {},
   "source": [
    "# Figures for cs182/282 Final Project\n",
    "\n",
    "jenniferyjlin@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a6425",
   "metadata": {},
   "source": [
    "# Load libriary and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38ba297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (23, 53)\n",
      "\n",
      "Column names:\n",
      "['Model', 'Pretraining', 'Input length', 'Classifier head', 'Classifier head -  hidden layers', 'Embedding strategy', 'Embedding  Strategy ', 'Fine-tuning', 'LoRA rank', 'Learning rate', 'Training Sample size', 'Batch size', 'Num steps', 'Grad accum', 'Best Validation AUC', 'Training time (hr)', 'Training memory (GB)', 'Trainable params', 'All params', 'Training_Step1000_AUC', 'Training_Step2000_AUC', 'Training_Step3000_AUC', 'Training_Step4000_AUC', 'Training_Step5000_AUC', 'Training_Step6000_AUC', 'Training_Step7000_AUC', 'Training_Step8000_AUC', 'Training_Step9000_AUC', 'Training_Step10000_AUC', 'Training_Step11000_AUC', 'Training_Step12000_AUC', 'Training_Step13000_AUC', 'Training_Step14000_AUC', 'Val_Step1000_AUC', 'Val_Step2000_AUC', 'Val_Step3000_AUC', 'Val_Step4000_AUC', 'Val_Step5000_AUC', 'Val_Step6000_AUC', 'Val_Step7000_AUC', 'Val_Step8000_AUC', 'Val_Step9000_AUC', 'Val_Step10000_AUC', 'Val_Step11000_AUC', 'Val_Step12000_AUC', 'Val_Step13000_AUC', 'Val_Step14000_AUC', 'Unnamed: 47', 'Model source', 'Code', 'Run by', 'GPU ID', 'Note']\n",
      "\n",
      "First few rows:\n",
      "  Model    Pretraining  Input length Classifier head  \\\n",
      "0  NT-2  multi-species         12000             mlp   \n",
      "1  NT-2  multi-species         12000             mlp   \n",
      "2  NT-2  multi-species         12000             mlp   \n",
      "3  NT-2  multi-species         12000     transformer   \n",
      "4  NT-2  multi-species         12000     transformer   \n",
      "\n",
      "   Classifier head -  hidden layers Embedding strategy  \\\n",
      "0                                 2   variant_position   \n",
      "1                                 2          mean_pool   \n",
      "2                                 3   variant_position   \n",
      "3                                 2   variant_position   \n",
      "4                                 2          mean_pool   \n",
      "\n",
      "                                Embedding  Strategy  Fine-tuning  LoRA rank  \\\n",
      "0  Embedding at the variant position (batch_size,...      frozen        NaN   \n",
      "1  Sequence-level (mean pool) (batch_size, hidden...      frozen        NaN   \n",
      "2  Embedding at the variant position (batch_size,...      frozen        NaN   \n",
      "3  Full (batch_size, seq_length, hidden_dim) + va...      frozen        NaN   \n",
      "4  Full (batch_size, seq_length, hidden_dim) + me...      frozen        NaN   \n",
      "\n",
      "   Learning rate  ... Val_Step11000_AUC  Val_Step12000_AUC  Val_Step13000_AUC  \\\n",
      "0        0.00003  ...               NaN                NaN                NaN   \n",
      "1        0.00003  ...               NaN                NaN                NaN   \n",
      "2        0.00003  ...               NaN                NaN                NaN   \n",
      "3        0.00003  ...               NaN                NaN                NaN   \n",
      "4        0.00003  ...               NaN                NaN                NaN   \n",
      "\n",
      "  Val_Step14000_AUC  Unnamed: 47  \\\n",
      "0               NaN          NaN   \n",
      "1               NaN          NaN   \n",
      "2               NaN          NaN   \n",
      "3               NaN          NaN   \n",
      "4               NaN          NaN   \n",
      "\n",
      "                                        Model source             Code Run by  \\\n",
      "0  InstaDeepAI/nucleotide-transformer-v2-500m-mul...  NT-multi-phase1   Yaqi   \n",
      "1  InstaDeepAI/nucleotide-transformer-v2-500m-mul...  NT-multi-phase1   Yaqi   \n",
      "2  InstaDeepAI/nucleotide-transformer-v2-500m-mul...  NT-multi-phase1   Yaqi   \n",
      "3  InstaDeepAI/nucleotide-transformer-v2-500m-mul...  NT-multi-phase1   Yaqi   \n",
      "4  InstaDeepAI/nucleotide-transformer-v2-500m-mul...  NT-multi-phase1   Yaqi   \n",
      "\n",
      "  GPU ID  Note  \n",
      "0    NaN   NaN  \n",
      "1    NaN   NaN  \n",
      "2    NaN   NaN  \n",
      "3    NaN   NaN  \n",
      "4    NaN   NaN  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "\n",
      "Model types:\n",
      "NT-2        15\n",
      "Caduceus     7\n",
      "NT-1         1\n",
      "Name: Model, dtype: int64\n",
      "\n",
      "Fine-tuning types:\n",
      "frozen                  15\n",
      "lora                     6\n",
      "unfreeze all             1\n",
      "frozen-input_leng=6k     1\n",
      "Name: Fine-tuning, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "output_dir = '../output/figures/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the combined data file\n",
    "df = pd.read_csv('../output/Results - 20251211 NT+Cad for report 3.tsv', sep='\\t')\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nModel types:\")\n",
    "print(df['Model'].value_counts())\n",
    "print(\"\\nFine-tuning types:\")\n",
    "print(df['Fine-tuning'].value_counts())\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341cf8d",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a9de84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized embedding strategies:\n",
      "variant_position    17\n",
      "mean_pool            6\n",
      "Name: Embedding_std, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define marker styles for each model\n",
    "MARKERS = {\n",
    "    'NT-2': 'o',      # Circle\n",
    "    'NT-1': '^',      # Triangle\n",
    "    'Caduceus': 'x',  # X\n",
    "    'NT-2-6k': 's'    # Square for 6k input length\n",
    "}\n",
    "\n",
    "# Standardize column names - handle both variations\n",
    "def get_classifier_layers(row):\n",
    "    \"\"\"Get classifier layers handling both column name variations\"\"\"\n",
    "    if pd.notna(row.get('Classifier head -  hidden layers')):\n",
    "        return row['Classifier head -  hidden layers']\n",
    "    elif pd.notna(row.get('Classifier head - hidden layers')):\n",
    "        return row['Classifier head - hidden layers']\n",
    "    return None\n",
    "\n",
    "# Add standardized columns\n",
    "df['Classifier_layers'] = df.apply(get_classifier_layers, axis=1)\n",
    "\n",
    "# Function to extract training/val curves\n",
    "def extract_curves(row, prefix='Val_Step', max_steps=14000):\n",
    "    steps = []\n",
    "    aucs = []\n",
    "    for step in range(1000, max_steps+1, 1000):\n",
    "        col = f'{prefix}{step}_AUC'\n",
    "        if col in row.index:\n",
    "            val = row[col]\n",
    "            # Check if value exists and is not NaN\n",
    "            if pd.notna(val):\n",
    "                try:\n",
    "                    auc = float(val)\n",
    "                    steps.append(step)\n",
    "                    aucs.append(auc)\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "    return np.array(steps), np.array(aucs)\n",
    "\n",
    "# Standardize embedding strategy\n",
    "def standardize_embedding(embed_str):\n",
    "    \"\"\"Standardize embedding strategy by removing 'full-' prefix and categorizing\"\"\"\n",
    "    if pd.isna(embed_str):\n",
    "        return 'unknown'\n",
    "    \n",
    "    embed_lower = str(embed_str).lower().strip()\n",
    "    \n",
    "    # Remove 'full-' prefix if present\n",
    "    if embed_lower.startswith('full-'):\n",
    "        embed_lower = embed_lower.replace('full-', '', 1)\n",
    "    \n",
    "    # Check downsample FIRST (most specific patterns)\n",
    "    if 'downsample' in embed_lower:\n",
    "        if 'mean' in embed_lower:\n",
    "            return 'mean_pool' # 'downsample_mean'\n",
    "        elif 'variant' in embed_lower:\n",
    "            return 'variant_position' #'downsample_variant'\n",
    "        else:\n",
    "            return 'downsample'\n",
    "    # Then check variant position (less specific)\n",
    "    elif 'variant' in embed_lower:\n",
    "        return 'variant_position'\n",
    "    # Then check mean pool\n",
    "    elif 'mean' in embed_lower:\n",
    "        return 'mean_pool'\n",
    "    else:\n",
    "        return embed_str\n",
    "\n",
    "df['Embedding_std'] = df['Embedding strategy'].apply(standardize_embedding)\n",
    "\n",
    "print(\"\\nStandardized embedding strategies:\")\n",
    "print(df['Embedding_std'].value_counts())\n",
    "\n",
    "# Filter data by model and fine-tuning status\n",
    "def filter_models(df, model_name, finetuning_status):\n",
    "    \"\"\"Filter models by name and fine-tuning status\"\"\"\n",
    "    mask = (df['Model'] == model_name) & (df['Fine-tuning'].str.contains(finetuning_status, na=False))\n",
    "    return df[mask].copy()\n",
    "\n",
    "# Function to extract training/val curves\n",
    "def extract_curves(row, prefix='Val_Step', max_steps=14000):\n",
    "    steps = []\n",
    "    aucs = []\n",
    "    for step in range(1000, max_steps+1, 1000):\n",
    "        col = f'{prefix}{step}_AUC'\n",
    "        if col in row.index:\n",
    "            val = row[col]\n",
    "            # Check if value exists and is not NaN\n",
    "            if pd.notna(val):\n",
    "                try:\n",
    "                    auc = float(val)\n",
    "                    steps.append(step)\n",
    "                    aucs.append(auc)\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "    return np.array(steps), np.array(aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfef79",
   "metadata": {},
   "source": [
    "# Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694e59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIGURE 2: Classifier Head and Embedding Strategy Comparison (NT-2, NT-1, Caduceus)\n",
      "================================================================================\n",
      "\n",
      "Frozen NT-2 models: 8\n",
      "Frozen NT-1 models: 1\n",
      "Frozen Caduceus models: 7\n",
      "\n",
      "Saved fig2_classifier_embedding_comparison.pdf/png\n",
      "  ✓ Plotted 16 models total\n",
      "  ✓ NT-2: 8 models\n",
      "  ✓ NT-1: 1 models\n",
      "  ✓ Caduceus: 7 models\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIGURE 2: Classifier Head and Embedding Strategy Comparison (NT-2, NT-1, Caduceus)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get frozen models for all model types\n",
    "frozen_nt2 = filter_models(df, 'NT-2', 'frozen')\n",
    "frozen_nt1 = filter_models(df, 'NT-1', 'frozen')\n",
    "frozen_cad = filter_models(df, 'Caduceus', 'frozen')\n",
    "\n",
    "print(f\"\\nFrozen NT-2 models: {len(frozen_nt2)}\")\n",
    "print(f\"Frozen NT-1 models: {len(frozen_nt1)}\")\n",
    "print(f\"Frozen Caduceus models: {len(frozen_cad)}\")\n",
    "\n",
    "# Organize models by embedding strategy\n",
    "def organize_by_embedding(df_subset, model_type):\n",
    "    \"\"\"Organize models by embedding strategy and return categorized lists\"\"\"\n",
    "    models_by_embedding = {\n",
    "        'variant_position': [],\n",
    "        'mean_pool': [],\n",
    "        'downsample_mean': [],\n",
    "        'downsample_variant': []\n",
    "    }\n",
    "    \n",
    "    for idx, row in df_subset.iterrows():\n",
    "        embed_std = row['Embedding_std']\n",
    "        head = row['Classifier head']\n",
    "        layers = row['Classifier_layers']\n",
    "        input_length = row.get('Input length', 12000)  # Get input length, default 12000\n",
    "        \n",
    "        if embed_std in models_by_embedding:\n",
    "            models_by_embedding[embed_std].append((idx, head, layers, model_type, input_length))\n",
    "    \n",
    "    return models_by_embedding\n",
    "\n",
    "# Organize all frozen models\n",
    "nt2_models = organize_by_embedding(frozen_nt2, 'NT-2')\n",
    "nt1_models = organize_by_embedding(frozen_nt1, 'NT-1')\n",
    "cad_models = organize_by_embedding(frozen_cad, 'Caduceus')\n",
    "\n",
    "# Assign colors based on classifier head and layers\n",
    "def get_color_for_config(head, layers, embed_type):\n",
    "    \"\"\"Get color based on architecture configuration\"\"\"\n",
    "    head_lower = str(head).lower() if pd.notna(head) else 'unknown'\n",
    "    \n",
    "    # Special handling for downsample embeddings\n",
    "    if 'downsample' in embed_type:\n",
    "        if head_lower == 'cnn':\n",
    "            return 'purple'\n",
    "        elif head_lower == 'transformer':\n",
    "            return 'darkviolet'\n",
    "        else:\n",
    "            return 'mediumorchid'\n",
    "    \n",
    "    # Standard colors for other embeddings\n",
    "    if head_lower == 'mlp':\n",
    "        if str(layers) == '2':\n",
    "            return 'skyblue' if embed_type == 'variant_position' else 'lightcoral'\n",
    "        else:\n",
    "            return 'C0' if embed_type == 'variant_position' else 'C1'\n",
    "    elif head_lower == 'transformer':\n",
    "        return 'C2' if embed_type == 'variant_position' else 'C3'\n",
    "    elif head_lower == 'cnn':\n",
    "        return 'C4' if embed_type == 'variant_position' else 'C5'\n",
    "    return 'C6'\n",
    "\n",
    "# Create Figure 1\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "gs = fig.add_gridspec(1, 3, width_ratios=[1, 1, 0.35], wspace=0.3)\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "\n",
    "# Store all plotted models for legend\n",
    "plotted_models = []\n",
    "\n",
    "# Plot function for a set of models\n",
    "def plot_models(models_list, ax1, ax2, embed_type, linestyle='-', alpha=1.0):\n",
    "    \"\"\"Plot training and validation curves for a list of models\"\"\"\n",
    "    for idx, head, layers, model, input_length in models_list:\n",
    "        row = df.loc[idx]\n",
    "        steps_train, aucs_train = extract_curves(row, 'Training_Step', 5000)\n",
    "        steps_val, aucs_val = extract_curves(row, 'Val_Step', 5000)\n",
    "        \n",
    "        color = get_color_for_config(head, layers, embed_type)\n",
    "        \n",
    "        # Determine marker based on model type and input length\n",
    "        if model == 'NT-1':\n",
    "            # NT-1 always uses triangle marker regardless of input length\n",
    "            marker = MARKERS.get('NT-1', '^')\n",
    "            input_label = ''\n",
    "        elif model == 'NT-2' and input_length == 6000:\n",
    "            # NT-2 with 6k input uses square marker\n",
    "            marker = MARKERS.get('NT-2-6k', 's')\n",
    "            input_label = ' (6k)'\n",
    "        else:\n",
    "            # Default marker for the model\n",
    "            marker = MARKERS.get(model, 'o')\n",
    "            input_label = ''\n",
    "        \n",
    "        # Create label\n",
    "        head_str = str(head).upper() if pd.notna(head) else 'UNK'\n",
    "        layers_str = str(int(layers)) if pd.notna(layers) else '?'\n",
    "        \n",
    "        if embed_type == 'variant_position':\n",
    "            embed_label = 'Var'\n",
    "        elif embed_type == 'mean_pool':\n",
    "            embed_label = 'Mean'\n",
    "        elif embed_type == 'downsample_mean':\n",
    "            embed_label = 'Down+Mean'\n",
    "        elif embed_type == 'downsample_variant':\n",
    "            embed_label = 'Down+Var'\n",
    "        else:\n",
    "            embed_label = embed_type[:4].capitalize()\n",
    "        \n",
    "        if model == 'Caduceus':\n",
    "            model_label = 'Cad'\n",
    "        else:\n",
    "            model_label = model\n",
    "            \n",
    "        label = f'{model_label}: {head_str}-{layers_str}L + {embed_label}{input_label}'\n",
    "        \n",
    "        # Get validation AUC at step 5000 for sorting\n",
    "        val_auc_5000 = aucs_val[-1] if len(aucs_val) > 0 else 0\n",
    "        \n",
    "        markersize = 7 if model != 'Caduceus' else 6\n",
    "        \n",
    "        if len(steps_train) > 0:\n",
    "            ax1.plot(steps_train, aucs_train, marker=marker, color=color, \n",
    "                    linewidth=2.5, linestyle=linestyle, markersize=markersize, alpha=alpha)\n",
    "        if len(steps_val) > 0:\n",
    "            line, = ax2.plot(steps_val, aucs_val, marker=marker, color=color, \n",
    "                           linewidth=2.5, linestyle=linestyle, markersize=markersize, alpha=alpha)\n",
    "            plotted_models.append((val_auc_5000, label, line))\n",
    "\n",
    "# Plot variant position models (solid lines)\n",
    "for models_dict, alpha_val in [(nt2_models, 1.0), (nt1_models, 1.0), (cad_models, 0.8)]:\n",
    "    if models_dict['variant_position']:\n",
    "        plot_models(models_dict['variant_position'], ax1, ax2, 'variant_position', \n",
    "                   linestyle='-', alpha=alpha_val)\n",
    "\n",
    "# Plot mean pooling models (dashed lines)\n",
    "for models_dict, alpha_val in [(nt2_models, 1.0), (cad_models, 0.8)]:\n",
    "    if models_dict['mean_pool']:\n",
    "        plot_models(models_dict['mean_pool'], ax1, ax2, 'mean_pool', \n",
    "                   linestyle='--', alpha=alpha_val)\n",
    "\n",
    "# Plot downsample models (dotted lines)\n",
    "for models_dict, alpha_val in [(cad_models, 0.8)]:\n",
    "    if models_dict['downsample_mean']:\n",
    "        plot_models(models_dict['downsample_mean'], ax1, ax2, 'downsample_mean', \n",
    "                   linestyle=':', alpha=alpha_val)\n",
    "    if models_dict['downsample_variant']:\n",
    "        plot_models(models_dict['downsample_variant'], ax1, ax2, 'downsample_variant', \n",
    "                   linestyle=':', alpha=alpha_val)\n",
    "\n",
    "# Sort legend by validation AUC\n",
    "plotted_models.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Configure axes\n",
    "ax1.set_xlabel('Training Step', fontsize=20, fontweight='bold')\n",
    "ax1.set_ylabel('Training AUC', fontsize=20, fontweight='bold')\n",
    "ax1.set_title('Training Performance', fontsize=20, fontweight='bold')\n",
    "ax1.set_ylim([0.4, 1.0])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Validation Step', fontsize=20, fontweight='bold')\n",
    "ax2.set_ylabel('Validation AUC', fontsize=20, fontweight='bold')\n",
    "ax2.set_title('Validation Performance', fontsize=20, fontweight='bold')\n",
    "ax2.set_ylim([0.4, 1.0])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add performance tier annotations on validation plot\n",
    "ax2.text(5300, 0.875, 'NT-2: Var', \n",
    "         fontsize=16, bbox=dict(boxstyle='round,pad=0.4', facecolor='lightblue', \n",
    "         alpha=0.7, edgecolor='navy'), verticalalignment='center')\n",
    "\n",
    "ax2.text(5300, 0.78, 'NT-2: Mean', \n",
    "         fontsize=16, bbox=dict(boxstyle='round,pad=0.4', facecolor='lightcoral', \n",
    "         alpha=0.7, edgecolor='darkred'), verticalalignment='center')\n",
    "\n",
    "ax2.text(5300, 0.735, 'NT-1: Var', \n",
    "         fontsize=16, bbox=dict(boxstyle='round,pad=0.4', facecolor='gray', \n",
    "         alpha=0.7, edgecolor='black'), verticalalignment='center')\n",
    "\n",
    "ax2.text(5300, 0.6, 'Cad: Var', \n",
    "         fontsize=16, bbox=dict(boxstyle='round,pad=0.4', facecolor='lightblue', \n",
    "         alpha=0.7, edgecolor='navy'), verticalalignment='center')\n",
    "\n",
    "ax2.text(5300, 0.45, 'Cad: Mean', \n",
    "         fontsize=16, bbox=dict(boxstyle='round,pad=0.4', facecolor='lightcoral', \n",
    "         alpha=0.7, edgecolor='darkred'), verticalalignment='center')\n",
    "\n",
    "# Create custom legend elements for markers (models)\n",
    "marker_legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='black', linestyle='None', markersize=8, label='NT-2 (12k)'),\n",
    "    Line2D([0], [0], marker='s', color='black', linestyle='None', markersize=8, label='NT-2 (6k)'),\n",
    "    Line2D([0], [0], marker='^', color='black', linestyle='None', markersize=8, label='NT-1 (6k)'),\n",
    "    Line2D([0], [0], marker='x', color='black', linestyle='None', markersize=6, label='Caduceus (30k)'),\n",
    "]\n",
    "\n",
    "# Create custom legend elements for line styles (embedding strategies)\n",
    "linestyle_legend_elements = [\n",
    "    Line2D([0], [0], color='black', linestyle='-', linewidth=2, label='Var pos'),\n",
    "    Line2D([0], [0], color='black', linestyle='--', linewidth=2, label='Mean pool')\n",
    "]\n",
    "\n",
    "# Add marker legend (aligned to same x position)\n",
    "marker_legend = fig.legend(handles=marker_legend_elements, loc='upper left', \n",
    "                          bbox_to_anchor=(0.82, 0.9), fontsize=16, \n",
    "                          title='Model', title_fontsize=16, frameon=True, \n",
    "                          fancybox=True, shadow=True)\n",
    "\n",
    "# Add linestyle legend (aligned to same x position)\n",
    "linestyle_legend = fig.legend(handles=linestyle_legend_elements, loc='upper left', \n",
    "                             bbox_to_anchor=(0.97, 0.9), fontsize=16, # 0.82, 0.775\n",
    "                             title='Embedding Strategy', title_fontsize=16, \n",
    "                             frameon=True, fancybox=True, shadow=True)\n",
    "\n",
    "fig.legend([x[2] for x in plotted_models], [f'{x[1]} ({x[0]:.3f})' for x in plotted_models], \n",
    "          loc='center left', bbox_to_anchor=(0.82, 0.32), fontsize=16, framealpha=0.9)\n",
    "\n",
    "\n",
    "plt.suptitle('Frozen Model Comparison: NT-2, NT-1, and Caduceus', \n",
    "            fontsize=24, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig(output_dir + 'fig2_classifier_embedding_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir + 'fig2_classifier_embedding_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nSaved fig2_classifier_embedding_comparison.pdf/png\")\n",
    "print(f\"  ✓ Plotted {len(plotted_models)} models total\")\n",
    "print(f\"  ✓ NT-2: {len(nt2_models['variant_position']) + len(nt2_models['mean_pool'])} models\")\n",
    "print(f\"  ✓ NT-1: {len(nt1_models['variant_position'])} models\")\n",
    "print(f\"  ✓ Caduceus: {sum(len(v) for v in cad_models.values())} models\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d572dee",
   "metadata": {},
   "source": [
    "# Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa7c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIGURE 3: LoRA Rank Comparison\n",
      "================================================================================\n",
      "Saved fig3_lora_rank_comparison.pdf/png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIGURE 3: LoRA Rank Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Figure 3: LoRA rank comparison (same as before but with same y-range)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Filter for rows with Num steps == 4000\n",
    "df_filtered = df[df['Num steps'] == 4000]\n",
    "\n",
    "lora_models = [\n",
    "    (8, 'LoRA rank=8', 'C0'),\n",
    "    (16, 'LoRA rank=16', 'C1'),\n",
    "    (32, 'LoRA rank=32', 'C2'),\n",
    "]\n",
    "\n",
    "for rank, label, color in lora_models:\n",
    "    # Select row with specific LoRA rank\n",
    "    row_data = df_filtered[df_filtered['LoRA rank'] == rank]\n",
    "    \n",
    "    if len(row_data) > 0:\n",
    "        row = row_data.iloc[0]  # Get the first matching row\n",
    "        \n",
    "        # Training curve\n",
    "        steps_train, aucs_train = extract_curves(row, 'Training_Step', 4000)\n",
    "        # Validation curve\n",
    "        steps_val, aucs_val = extract_curves(row, 'Val_Step', 4000)\n",
    "        \n",
    "        if len(steps_train) > 0:\n",
    "            ax1.plot(steps_train, aucs_train, marker='o', label=label, color=color, linewidth=2.5)\n",
    "        if len(steps_val) > 0:\n",
    "            ax2.plot(steps_val, aucs_val, marker='o', label=label, color=color, linewidth=2.5)\n",
    "\n",
    "# Set same y-range\n",
    "y_min = 0.81\n",
    "y_max = 0.99\n",
    "ax1.set_ylim([y_min, y_max])\n",
    "ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "ax1.set_xlabel('Training Steps', fontsize=11)\n",
    "ax1.set_ylabel('Training AUC', fontsize=11)\n",
    "ax1.set_title('Training Performance', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Training Steps', fontsize=11)\n",
    "ax2.set_ylabel('Validation AUC', fontsize=11)\n",
    "ax2.set_title('Validation Performance', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('LoRA Rank Comparison (5k samples, CNN + Variant pos.)', fontsize=13, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + 'fig3_lora_rank_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir + 'fig3_lora_rank_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved fig3_lora_rank_comparison.pdf/png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de718f4e",
   "metadata": {},
   "source": [
    "# Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a76736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIGURE 4: Learning Rate Comparison\n",
      "================================================================================\n",
      "Saved fig4_learning_rate_comparison.pdf/png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIGURE 4: Learning Rate Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Figure 4: Learning rate comparison with training and validation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Filter for rows with LoRA rank == 32 and Num steps == 40000\n",
    "df_filtered = df[(df['LoRA rank'] == 32) & (df['Num steps'] == 40000)]\n",
    "\n",
    "lr_models = [\n",
    "    (0.00003, '3e-5', 'C0'),\n",
    "    (0.00005, '5e-5', 'C1'),\n",
    "    (0.00001, '1e-5', 'C2'),\n",
    "]\n",
    "\n",
    "for lr_value, lr_label, color in lr_models:\n",
    "    # Select row with specific learning rate\n",
    "    row_data = df_filtered[df_filtered['Learning rate'] == lr_value]\n",
    "    \n",
    "    if len(row_data) > 0:\n",
    "        row = row_data.iloc[0]  # Get the first matching row\n",
    "        \n",
    "        # Training curves\n",
    "        steps_train, aucs_train = extract_curves(row, 'Training_Step', 14000)\n",
    "        # Validation curves\n",
    "        steps_val, aucs_val = extract_curves(row, 'Val_Step', 14000)\n",
    "        \n",
    "        if len(steps_train) > 0:\n",
    "            ax1.plot(steps_train, aucs_train, marker='o', label=f'LR={lr_label}', color=color, linewidth=2.5)\n",
    "        if len(steps_val) > 0:\n",
    "            ax2.plot(steps_val, aucs_val, marker='o', label=f'LR={lr_label}', color=color, linewidth=2.5)\n",
    "\n",
    "# Set same y-range\n",
    "y_min = 0.7\n",
    "y_max = 0.97\n",
    "ax1.set_ylim([y_min, y_max])\n",
    "ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "ax1.set_xlabel('Training Steps', fontsize=11)\n",
    "ax1.set_ylabel('Training AUC', fontsize=11)\n",
    "ax1.set_title('Training Performance', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Training Steps', fontsize=11)\n",
    "ax2.set_ylabel('Validation AUC', fontsize=11)\n",
    "ax2.set_title('Validation Performance', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Learning Rate Optimization (44k samples, LoRA rank=32)', fontsize=13, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + 'fig4_learning_rate_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir + 'fig4_learning_rate_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved fig4_learning_rate_comparison.pdf/png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62b7d6",
   "metadata": {},
   "source": [
    "# Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8313fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIGURE 5: Best LoRA vs Full Fine-tuning (Overfitting Analysis)\n",
      "================================================================================\n",
      "Best frozen model: index 6, Val AUC = 0.8710\n",
      "Best LoRA model: index 12, Val AUC = 0.8861\n",
      "Unfrozen model: index 7, Val AUC = 0.7880\n",
      "Saved fig5_lora_vs_full_finetuning.pdf/png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIGURE 5: Best LoRA vs Full Fine-tuning (Overfitting Analysis)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Figure 5: Best LoRA vs unfrozen (overfitting comparison)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Best frozen model (baseline) - highest validation AUC among Num steps == 5000\n",
    "df_frozen = df[df['Num steps'] == 5000]\n",
    "frozen_idx = df_frozen['Best Validation AUC'].idxmax()\n",
    "frozen_row = df.loc[frozen_idx]\n",
    "print(f\"Best frozen model: index {frozen_idx}, Val AUC = {frozen_row['Best Validation AUC']:.4f}\")\n",
    "\n",
    "steps_train, aucs_train = extract_curves(frozen_row, 'Training_Step', 5000)\n",
    "steps_val, aucs_val = extract_curves(frozen_row, 'Val_Step', 5000)\n",
    "if len(steps_train) > 0:\n",
    "    ax1.plot(steps_train, aucs_train, marker='o', label='Frozen baseline', color='C0', linewidth=2.5, linestyle='-')\n",
    "if len(steps_val) > 0:\n",
    "    ax2.plot(steps_val, aucs_val, marker='o', label='Frozen baseline', color='C0', linewidth=2.5, linestyle='-')\n",
    "\n",
    "# Best LoRA model - highest validation AUC among LoRA rank=32, Num steps=40000\n",
    "df_lora = df[(df['LoRA rank'] == 32) & (df['Num steps'] == 40000)]\n",
    "lora_idx = df_lora['Best Validation AUC'].idxmax()\n",
    "lora_row = df.loc[lora_idx]\n",
    "print(f\"Best LoRA model: index {lora_idx}, Val AUC = {lora_row['Best Validation AUC']:.4f}\")\n",
    "\n",
    "steps_train, aucs_train = extract_curves(lora_row, 'Training_Step', 14000)\n",
    "steps_val, aucs_val = extract_curves(lora_row, 'Val_Step', 14000)\n",
    "if len(steps_train) > 0:\n",
    "    ax1.plot(steps_train, aucs_train, marker='s', label='LoRA (rank=32, LR=3e-5)', color='C2', linewidth=2.5, linestyle='-')\n",
    "if len(steps_val) > 0:\n",
    "    ax2.plot(steps_val, aucs_val, marker='s', label='LoRA (rank=32, LR=3e-5)', color='C2', linewidth=2.5, linestyle='-')\n",
    "\n",
    "# Unfrozen model (full fine-tuning)\n",
    "df_unfrozen = df[df['Fine-tuning'] == 'unfreeze all']\n",
    "unfreeze_idx = df_unfrozen.index[0]  # Get the first (and likely only) unfrozen model\n",
    "unfreeze_row = df.loc[unfreeze_idx]\n",
    "print(f\"Unfrozen model: index {unfreeze_idx}, Val AUC = {unfreeze_row['Best Validation AUC']:.4f}\")\n",
    "\n",
    "steps_train, aucs_train = extract_curves(unfreeze_row, 'Training_Step', 10000)\n",
    "steps_val, aucs_val = extract_curves(unfreeze_row, 'Val_Step', 10000)\n",
    "if len(steps_train) > 0:\n",
    "    ax1.plot(steps_train, aucs_train, marker='^', label='Full fine-tuning (unfrozen)', color='C3', linewidth=2.5, linestyle='--', alpha=0.8)\n",
    "if len(steps_val) > 0:\n",
    "    ax2.plot(steps_val, aucs_val, marker='^', label='Full fine-tuning (unfrozen)', color='C3', linewidth=2.5, linestyle='--', alpha=0.8)\n",
    "\n",
    "# Set same y-range\n",
    "y_min = 0.5\n",
    "y_max = 1\n",
    "ax1.set_ylim([y_min, y_max])\n",
    "ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "ax1.set_xlabel('Training Steps', fontsize=11)\n",
    "ax1.set_ylabel('Training AUC', fontsize=11)\n",
    "ax1.set_title('Training Performance', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Training Steps', fontsize=11)\n",
    "ax2.set_ylabel('Validation AUC', fontsize=11)\n",
    "ax2.set_title('Validation Performance', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Generalization: LoRA vs Full Fine-tuning', fontsize=13, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + 'fig5_lora_vs_full_finetuning.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir + 'fig5_lora_vs_full_finetuning.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved fig5_lora_vs_full_finetuning.pdf/png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a03631",
   "metadata": {},
   "source": [
    "# Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4f2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIGURE 6: Model Performance Summary\n",
      "================================================================================\n",
      "\n",
      "Total models in summary: 23\n",
      "\n",
      "Saved fig6_model_performance_summary.pdf/png\n",
      "  ✓ 23 models in progressive optimization pipeline\n",
      "  ✓ Best model: LR=3e-5 (AUC = 0.886)\n",
      "  ✓ Stage 1: Mean pool (gray), Var pos (blue), 6k input (purple/silver)\n",
      "  ✓ Stage 2: LoRA ranks (green gradient)\n",
      "  ✓ Stage 3: Learning rates (warm gradient)\n",
      "  ✓ Stage 4: Full FT (red)\n",
      "\n",
      "================================================================================\n",
      "Processing complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIGURE 6: Model Performance Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build model data systematically from the dataframe\n",
    "model_data = []\n",
    "\n",
    "def add_model_to_summary(row, short_name):\n",
    "    \"\"\"Extract model information and add to summary\"\"\"\n",
    "    best_auc = row['Best Validation AUC']\n",
    "    model_type = row['Model']\n",
    "    classifier = str(row['Classifier head']).upper() if pd.notna(row['Classifier head']) else 'UNK'\n",
    "    layers = str(int(row['Classifier_layers'])) if pd.notna(row['Classifier_layers']) else '?'\n",
    "    \n",
    "    # Determine embedding type\n",
    "    embed_std = row['Embedding_std']\n",
    "    if embed_std == 'variant_position':\n",
    "        embedding = 'Var'\n",
    "    elif embed_std == 'mean_pool':\n",
    "        embedding = 'Mean'\n",
    "    elif 'downsample' in embed_std:\n",
    "        embedding = 'Down'\n",
    "    else:\n",
    "        embedding = '?'\n",
    "    \n",
    "    # Fine-tuning status\n",
    "    finetuning_raw = row['Fine-tuning']\n",
    "    if 'lora' in str(finetuning_raw).lower():\n",
    "        finetuning = 'LoRA'\n",
    "    elif 'frozen' in str(finetuning_raw).lower():\n",
    "        finetuning = 'Frozen'\n",
    "    elif 'unfreeze' in str(finetuning_raw).lower():\n",
    "        finetuning = 'Full'\n",
    "    else:\n",
    "        finetuning = str(finetuning_raw)\n",
    "    \n",
    "    # LoRA rank\n",
    "    rank = str(int(row['LoRA rank'])) if pd.notna(row['LoRA rank']) else '-'\n",
    "    \n",
    "    # Learning rate\n",
    "    lr_val = row['Learning rate']\n",
    "    if pd.notna(lr_val):\n",
    "        lr = f\"{lr_val:.0e}\".replace('e-0', 'e-')\n",
    "    else:\n",
    "        lr = '-'\n",
    "    \n",
    "    # Sample size\n",
    "    samples_raw = row['Training Sample size']\n",
    "    if pd.notna(samples_raw):\n",
    "        if 'k' in str(samples_raw).lower():\n",
    "            samples = str(samples_raw)\n",
    "        else:\n",
    "            samples = f\"{int(samples_raw)/1000:.0f}k\"\n",
    "    else:\n",
    "        samples = '-'\n",
    "    \n",
    "    # Input length\n",
    "    input_length = row.get('Input length', '-')\n",
    "    if pd.notna(input_length):\n",
    "        input_len = f\"{int(input_length)/1000:.0f}k\"\n",
    "    else:\n",
    "        input_len = '-'\n",
    "    \n",
    "    # Shorten model name for Caduceus\n",
    "    if model_type == 'Caduceus':\n",
    "        model_display = 'Cad'\n",
    "    else:\n",
    "        model_display = model_type\n",
    "    \n",
    "    model_data.append((\n",
    "        short_name,\n",
    "        best_auc,\n",
    "        model_display,\n",
    "        classifier,\n",
    "        layers,\n",
    "        embedding,\n",
    "        finetuning,\n",
    "        rank,\n",
    "        lr,\n",
    "        samples,\n",
    "        input_len\n",
    "    ))\n",
    "\n",
    "# Track stage boundaries\n",
    "stage_boundaries = []\n",
    "\n",
    "# Stage 1: Mean pool - Caduceus (sorted by AUC ascending - lowest first)\n",
    "mean_pool_cad = frozen_cad[frozen_cad['Embedding_std'] == 'mean_pool'].copy()\n",
    "mean_pool_cad = mean_pool_cad.sort_values('Best Validation AUC')\n",
    "for idx, row in mean_pool_cad.iterrows():\n",
    "    head = str(row['Classifier head']).upper()\n",
    "    layers = int(row['Classifier_layers']) if pd.notna(row['Classifier_layers']) else 0\n",
    "    add_model_to_summary(row, f'Cad {head}-{layers}L Mean')\n",
    "\n",
    "# Stage 1: Mean pool - NT-2 (12k only, sorted by AUC ascending)\n",
    "mean_pool_nt2 = frozen_nt2[frozen_nt2['Embedding_std'] == 'mean_pool'].copy()\n",
    "mean_pool_nt2_12k = mean_pool_nt2[mean_pool_nt2['Input length'] == 12000]\n",
    "mean_pool_nt2_12k = mean_pool_nt2_12k.sort_values('Best Validation AUC')\n",
    "for idx, row in mean_pool_nt2_12k.iterrows():\n",
    "    head = str(row['Classifier head']).upper()\n",
    "    layers = int(row['Classifier_layers']) if pd.notna(row['Classifier_layers']) else 0\n",
    "    add_model_to_summary(row, f'{head}-{layers}L Mean')\n",
    "\n",
    "# Record boundary after mean pool\n",
    "stage_boundaries.append(('mean_pool', len(model_data) - 0.5))\n",
    "\n",
    "# Stage 1: Variant position - Caduceus (sorted by AUC ascending)\n",
    "var_pos_cad = frozen_cad[frozen_cad['Embedding_std'] == 'variant_position'].copy()\n",
    "var_pos_cad = var_pos_cad.sort_values('Best Validation AUC')\n",
    "for idx, row in var_pos_cad.iterrows():\n",
    "    head = str(row['Classifier head']).upper()\n",
    "    layers = int(row['Classifier_layers']) if pd.notna(row['Classifier_layers']) else 0\n",
    "    add_model_to_summary(row, f'Cad {head}-{layers}L Var')\n",
    "\n",
    "# Stage 1: Variant position - NT-2 (12k only, sorted by AUC ascending)\n",
    "var_pos_nt2 = frozen_nt2[frozen_nt2['Embedding_std'] == 'variant_position'].copy()\n",
    "var_pos_nt2_12k = var_pos_nt2[var_pos_nt2['Input length'] == 12000]\n",
    "var_pos_nt2_12k = var_pos_nt2_12k.sort_values('Best Validation AUC')\n",
    "for idx, row in var_pos_nt2_12k.iterrows():\n",
    "    head = str(row['Classifier head']).upper()\n",
    "    layers = int(row['Classifier_layers']) if pd.notna(row['Classifier_layers']) else 0\n",
    "    add_model_to_summary(row, f'{head}-{layers}L Var')\n",
    "\n",
    "# Record boundary after variant position\n",
    "stage_boundaries.append(('var_pos', len(model_data) - 0.5))\n",
    "\n",
    "# Stage 1: 6k input models (NT-2 with 6k and NT-1)\n",
    "# NT-2 with 6k input\n",
    "var_pos_nt2_6k = var_pos_nt2[var_pos_nt2['Input length'] == 6000]\n",
    "for idx, row in var_pos_nt2_6k.iterrows():\n",
    "    head = str(row['Classifier head']).upper()\n",
    "    layers = int(row['Classifier_layers']) if pd.notna(row['Classifier_layers']) else 0\n",
    "    add_model_to_summary(row, f'{head}-{layers}L (6k)')\n",
    "\n",
    "# NT-1\n",
    "for idx, row in frozen_nt1.iterrows():\n",
    "    add_model_to_summary(row, 'NT-1')\n",
    "\n",
    "# Record boundary after 6k models\n",
    "stage_boundaries.append(('6k', len(model_data) - 0.5))\n",
    "\n",
    "# Stage 2: LoRA ranks - get models with different ranks\n",
    "lora_models = df[df['Fine-tuning'].str.contains('lora', case=False, na=False)].copy()\n",
    "# Filter for the 5k training samples\n",
    "lora_5k = lora_models[lora_models['Training Sample size'].astype(str).str.contains('5k', na=False)]\n",
    "lora_5k = lora_5k.sort_values('LoRA rank')\n",
    "for idx, row in lora_5k.iterrows():\n",
    "    rank = int(row['LoRA rank'])\n",
    "    add_model_to_summary(row, f'LoRA r={rank}')\n",
    "\n",
    "# Record boundary after LoRA\n",
    "stage_boundaries.append(('lora', len(model_data) - 0.5))\n",
    "\n",
    "# Stage 3: Learning rates - get models with different LRs (44k samples)\n",
    "lora_44k = lora_models[lora_models['Training Sample size'].astype(str).str.contains('44k', na=False)]\n",
    "lora_44k = lora_44k.sort_values('Learning rate', ascending=True)  # Changed to ascending=True\n",
    "for idx, row in lora_44k.iterrows():\n",
    "    lr_val = row['Learning rate']\n",
    "    lr_str = f\"{lr_val:.0e}\".replace('e-0', 'e-')\n",
    "    add_model_to_summary(row, f'LR={lr_str}')\n",
    "\n",
    "# Record boundary after learning rates\n",
    "stage_boundaries.append(('lr', len(model_data) - 0.5))\n",
    "\n",
    "# Stage 4: Full fine-tuning\n",
    "full_ft = df[df['Fine-tuning'].str.contains('unfreeze', case=False, na=False)]\n",
    "for idx, row in full_ft.iterrows():\n",
    "    add_model_to_summary(row, 'Full FT')\n",
    "\n",
    "print(f\"\\nTotal models in summary: {len(model_data)}\")\n",
    "\n",
    "# Create Figure 5\n",
    "fig = plt.figure(figsize=(22, 11))\n",
    "gs = fig.add_gridspec(2, 1, height_ratios=[1, 0.4], hspace=0.3)\n",
    "ax = fig.add_subplot(gs[0])\n",
    "ax_table = fig.add_subplot(gs[1])\n",
    "ax_table.axis('off')\n",
    "\n",
    "# Define colors by stage and group\n",
    "colors = []\n",
    "lora_count = 0\n",
    "lr_count = 0\n",
    "\n",
    "# Define colors by stage and group\n",
    "colors = []\n",
    "lora_count = 0\n",
    "lr_count = 0\n",
    "\n",
    "# Define colors by stage and group\n",
    "colors = []\n",
    "lora_count = 0\n",
    "lr_count = 0\n",
    "\n",
    "for i, (name, _, model_type, _, _, embedding, finetuning, _, _, _, _) in enumerate(model_data):\n",
    "    if 'LR=' in name:\n",
    "        # Learning rate models in warm colors (lighter to darker)\n",
    "        lr_colors = ['lightsalmon', 'salmon', 'tomato']\n",
    "        colors.append(lr_colors[lr_count % len(lr_colors)])\n",
    "        lr_count += 1\n",
    "    elif finetuning == 'LoRA':\n",
    "        # LoRA models in green gradient (lighter to darker)\n",
    "        lora_colors = ['lightgreen', 'mediumseagreen', 'seagreen']\n",
    "        colors.append(lora_colors[lora_count % len(lora_colors)])\n",
    "        lora_count += 1\n",
    "    elif finetuning == 'Full':\n",
    "        # Full fine-tuning in red\n",
    "        colors.append('indianred')\n",
    "    elif embedding == 'Mean':\n",
    "        # All mean pool in light gray\n",
    "        colors.append('lightgray')\n",
    "    elif embedding == 'Var':\n",
    "        # All variant position in sky blue\n",
    "        colors.append('skyblue')\n",
    "    elif '6k' in name or model_type == 'NT-1':\n",
    "        # 6k input models in purple/silver\n",
    "        if model_type == 'NT-1':\n",
    "            colors.append('silver')\n",
    "        else:\n",
    "            colors.append('mediumpurple')\n",
    "    else:\n",
    "        # Fallback\n",
    "        colors.append('gray')\n",
    "        \n",
    "\n",
    "# Add hatching patterns\n",
    "hatches = []\n",
    "for i, (name, _, model_type, _, _, _, _, _, _, _, _) in enumerate(model_data):\n",
    "    if model_type == 'Cad':\n",
    "        hatches.append('xxx')\n",
    "    elif model_type == 'NT-1':\n",
    "        hatches.append('//')\n",
    "    else:\n",
    "        hatches.append('')\n",
    "\n",
    "positions = list(range(len(model_data)))\n",
    "names = [m[0] for m in model_data]\n",
    "aucs = [m[1] for m in model_data]\n",
    "\n",
    "bars = ax.bar(positions, aucs, color=colors, alpha=0.85, \n",
    "             edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Apply hatching\n",
    "for bar, hatch in zip(bars, hatches):\n",
    "    bar.set_hatch(hatch)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, auc) in enumerate(zip(bars, aucs)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.003,\n",
    "            f'{auc:.3f}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add vertical dividers\n",
    "for stage_name, boundary in stage_boundaries:\n",
    "    ax.axvline(x=boundary, color='dimgray', linestyle='--', linewidth=2.5, alpha=0.6)\n",
    "\n",
    "# Calculate stage label positions\n",
    "# Mean pool stage\n",
    "mean_pool_start = 0\n",
    "mean_pool_end = stage_boundaries[0][1]\n",
    "mean_pool_center = (mean_pool_start + mean_pool_end) / 2\n",
    "\n",
    "# Variant position stage\n",
    "var_pos_start = stage_boundaries[0][1]\n",
    "var_pos_end = stage_boundaries[1][1]\n",
    "var_pos_center = (var_pos_start + var_pos_end) / 2\n",
    "\n",
    "# 6k input stage\n",
    "input_6k_start = stage_boundaries[1][1]\n",
    "input_6k_end = stage_boundaries[2][1]\n",
    "input_6k_center = (input_6k_start + input_6k_end) / 2\n",
    "\n",
    "# LoRA stage\n",
    "lora_start = stage_boundaries[2][1]\n",
    "lora_end = stage_boundaries[3][1]\n",
    "lora_center = (lora_start + lora_end) / 2\n",
    "\n",
    "# Learning rate stage\n",
    "lr_start = stage_boundaries[3][1]\n",
    "lr_end = stage_boundaries[4][1]\n",
    "lr_center = (lr_start + lr_end) / 2\n",
    "\n",
    "# Full FT stage\n",
    "full_ft_start = stage_boundaries[4][1]\n",
    "full_ft_end = len(model_data) - 0.5\n",
    "full_ft_center = (full_ft_start + full_ft_end) / 2\n",
    "\n",
    "# Add stage labels\n",
    "ax.text(mean_pool_center, 0.93, 'Stage 1:\\nMean Pool', ha='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.5, edgecolor='gray'))\n",
    "\n",
    "ax.text(var_pos_center, 0.93, 'Stage 1:\\nVariant Pos', ha='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightblue', alpha=0.5, edgecolor='navy'))\n",
    "\n",
    "ax.text(input_6k_center, 0.93, 'Stage 1:\\n6k Input', ha='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='lavender', alpha=0.5, edgecolor='purple'))\n",
    "\n",
    "ax.text(lora_center, 0.93, 'Stage 2:\\nLoRA Rank', ha='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgreen', alpha=0.5, edgecolor='darkgreen'))\n",
    "\n",
    "ax.text(lr_center, 0.93, 'Stage 3:\\nLearning Rate', ha='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightsalmon', alpha=0.5, edgecolor='darkred'))\n",
    "\n",
    "ax.text(full_ft_center+0.5, 0.93, 'Stage 4:\\nFull FT', ha='center', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='mistyrose', alpha=0.5, edgecolor='darkred'))\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = aucs.index(max(aucs))\n",
    "bars[best_idx].set_linewidth(4)\n",
    "bars[best_idx].set_edgecolor('red')\n",
    "bars[best_idx].set_zorder(10)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(names, fontsize=10, rotation=45, ha='right', fontweight='bold')\n",
    "ax.set_ylabel('Best Validation AUC', fontsize=20, fontweight='bold')\n",
    "ax.set_title('Model Performance Summary: Progressive Optimization (NT-2, NT-1, Caduceus)', \n",
    "            fontsize=22, fontweight='bold', pad=20)\n",
    "ax.set_ylim([0.40, 0.99])\n",
    "ax.grid(True, axis='y', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "# Create annotation table\n",
    "table_data = []\n",
    "row_labels = ['Model Type', 'Classifier', 'Layers', 'Embedding', 'Fine-tuning', \n",
    "             'LoRA Rank', 'Learning Rate', 'Samples', 'Input Len']\n",
    "\n",
    "for i, (name, auc, model_type, classifier, layers, embedding, finetuning, rank, lr, samples, input_len) in enumerate(model_data):\n",
    "    col_data = [model_type, classifier, layers, embedding, finetuning, rank, lr, samples, input_len]\n",
    "    table_data.append(col_data)\n",
    "\n",
    "# Transpose\n",
    "table_data_transposed = list(map(list, zip(*table_data)))\n",
    "\n",
    "# Create table\n",
    "table = ax_table.table(cellText=table_data_transposed,\n",
    "                       rowLabels=row_labels,\n",
    "                       colLabels=[f'{i+1}' for i in range(len(model_data))],\n",
    "                       cellLoc='center',\n",
    "                       rowLoc='center',\n",
    "                       loc='center',\n",
    "                       bbox=[0, 0, 1, 1])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(8)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Color code table\n",
    "for i in range(len(model_data)):\n",
    "    table[(0, i)].set_facecolor(colors[i])\n",
    "    table[(0, i)].set_alpha(0.7)\n",
    "    table[(0, i)].set_text_props(weight='bold', fontsize=9)\n",
    "    \n",
    "    if i == best_idx:\n",
    "        table[(0, i)].set_edgecolor('red')\n",
    "        table[(0, i)].set_linewidth(3)\n",
    "\n",
    "# Style row labels\n",
    "for i in range(len(row_labels)):\n",
    "    table[(i+1, -1)].set_facecolor('lightgray')\n",
    "    table[(i+1, -1)].set_alpha(0.5)\n",
    "    table[(i+1, -1)].set_text_props(weight='bold', fontsize=9)\n",
    "\n",
    "# Add borders\n",
    "for key, cell in table.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    cell.set_linewidth(0.5)\n",
    "\n",
    "plt.savefig(output_dir + 'fig6_model_performance_summary.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir + 'fig6_model_performance_summary.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nSaved fig6_model_performance_summary.pdf/png\")\n",
    "print(f\"  ✓ {len(model_data)} models in progressive optimization pipeline\")\n",
    "print(f\"  ✓ Best model: {names[best_idx]} (AUC = {aucs[best_idx]:.3f})\")\n",
    "print(\"  ✓ Stage 1: Mean pool (gray), Var pos (blue), 6k input (purple/silver)\")\n",
    "print(\"  ✓ Stage 2: LoRA ranks (green gradient)\")\n",
    "print(\"  ✓ Stage 3: Learning rates (warm gradient)\")\n",
    "print(\"  ✓ Stage 4: Full FT (red)\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Processing complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161eb399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary Table:\n",
      "                 Model Name  Best Val AUC Model Type   Classifier Layers  \\\n",
      "0           Cad MLP-2L Mean      0.459180        Cad          MLP      2   \n",
      "1   Cad TRANSFORMER-2L Mean      0.560211        Cad  TRANSFORMER      2   \n",
      "2           Cad CNN-2L Mean      0.670923        Cad          CNN      2   \n",
      "3               MLP-2L Mean      0.751000       NT-2          MLP      2   \n",
      "4       TRANSFORMER-2L Mean      0.775000       NT-2  TRANSFORMER      2   \n",
      "5               CNN-2L Mean      0.778000       NT-2          CNN      2   \n",
      "6            Cad MLP-2L Var      0.596876        Cad          MLP      2   \n",
      "7            Cad MLP-3L Var      0.599565        Cad          MLP      3   \n",
      "8    Cad TRANSFORMER-2L Var      0.700477        Cad  TRANSFORMER      2   \n",
      "9            Cad CNN-2L Var      0.753605        Cad          CNN      2   \n",
      "10               MLP-2L Var      0.869000       NT-2          MLP      2   \n",
      "11               MLP-3L Var      0.869000       NT-2          MLP      3   \n",
      "12       TRANSFORMER-2L Var      0.869000       NT-2  TRANSFORMER      2   \n",
      "13               CNN-2L Var      0.871000       NT-2          CNN      2   \n",
      "14              CNN-2L (6k)      0.863800       NT-2          CNN      2   \n",
      "15                     NT-1      0.732200       NT-1          CNN      2   \n",
      "16                 LoRA r=8      0.834500       NT-2          CNN      2   \n",
      "17                LoRA r=16      0.837600       NT-2          CNN      2   \n",
      "18                LoRA r=32      0.845300       NT-2          CNN      2   \n",
      "19                  LR=1e-5      0.879300       NT-2          CNN      2   \n",
      "20                  LR=3e-5      0.886100       NT-2          CNN      2   \n",
      "21                  LR=5e-5      0.885600       NT-2          CNN      2   \n",
      "22                  Full FT      0.788000       NT-2          CNN      2   \n",
      "\n",
      "   Embedding Fine-tuning LoRA Rank Learning Rate Samples Input Length  \n",
      "0       Mean      Frozen         -          3e-5     44k          30k  \n",
      "1       Mean      Frozen         -          3e-5     44k          30k  \n",
      "2       Mean      Frozen         -          3e-5     44k          30k  \n",
      "3       Mean      Frozen         -          3e-5     44k          12k  \n",
      "4       Mean      Frozen         -          3e-5     44k          12k  \n",
      "5       Mean      Frozen         -          3e-5     44k          12k  \n",
      "6        Var      Frozen         -          3e-5     44k          30k  \n",
      "7        Var      Frozen         -          3e-5     44k          30k  \n",
      "8        Var      Frozen         -          3e-5     44k          30k  \n",
      "9        Var      Frozen         -          3e-5     44k          30k  \n",
      "10       Var      Frozen         -          3e-5     44k          12k  \n",
      "11       Var      Frozen         -          3e-5     44k          12k  \n",
      "12       Var      Frozen         -          3e-5     44k          12k  \n",
      "13       Var      Frozen         -          3e-5     44k          12k  \n",
      "14       Var      Frozen         -          3e-5     44k           6k  \n",
      "15       Var      Frozen         -          3e-5     44k           6k  \n",
      "16       Var        LoRA         8          3e-5      5k          12k  \n",
      "17       Var        LoRA        16          3e-5      5k          12k  \n",
      "18       Var        LoRA        32          3e-5      5k          12k  \n",
      "19       Var        LoRA        32          1e-5     44k          12k  \n",
      "20       Var        LoRA        32          3e-5     44k          12k  \n",
      "21       Var        LoRA        32          5e-5     44k          12k  \n",
      "22       Var        Full         -          3e-5     44k          12k  \n",
      "\n",
      "Saved table to ../output/figures/fig6_model_summary_table.csv\n",
      "\n",
      "Transposed Table (as shown in plot):\n",
      "Model Name    Cad MLP-2L Mean Cad TRANSFORMER-2L Mean Cad CNN-2L Mean  \\\n",
      "Best Val AUC          0.45918                0.560211        0.670923   \n",
      "Model Type                Cad                     Cad             Cad   \n",
      "Classifier                MLP             TRANSFORMER             CNN   \n",
      "Layers                      2                       2               2   \n",
      "Embedding                Mean                    Mean            Mean   \n",
      "Fine-tuning            Frozen                  Frozen          Frozen   \n",
      "LoRA Rank                   -                       -               -   \n",
      "Learning Rate            3e-5                    3e-5            3e-5   \n",
      "Samples                   44k                     44k             44k   \n",
      "Input Length              30k                     30k             30k   \n",
      "\n",
      "Model Name    MLP-2L Mean TRANSFORMER-2L Mean CNN-2L Mean Cad MLP-2L Var  \\\n",
      "Best Val AUC        0.751               0.775       0.778       0.596876   \n",
      "Model Type           NT-2                NT-2        NT-2            Cad   \n",
      "Classifier            MLP         TRANSFORMER         CNN            MLP   \n",
      "Layers                  2                   2           2              2   \n",
      "Embedding            Mean                Mean        Mean            Var   \n",
      "Fine-tuning        Frozen              Frozen      Frozen         Frozen   \n",
      "LoRA Rank               -                   -           -              -   \n",
      "Learning Rate        3e-5                3e-5        3e-5           3e-5   \n",
      "Samples               44k                 44k         44k            44k   \n",
      "Input Length          12k                 12k         12k            30k   \n",
      "\n",
      "Model Name    Cad MLP-3L Var Cad TRANSFORMER-2L Var Cad CNN-2L Var  ...  \\\n",
      "Best Val AUC        0.599565               0.700477       0.753605  ...   \n",
      "Model Type               Cad                    Cad            Cad  ...   \n",
      "Classifier               MLP            TRANSFORMER            CNN  ...   \n",
      "Layers                     3                      2              2  ...   \n",
      "Embedding                Var                    Var            Var  ...   \n",
      "Fine-tuning           Frozen                 Frozen         Frozen  ...   \n",
      "LoRA Rank                  -                      -              -  ...   \n",
      "Learning Rate           3e-5                   3e-5           3e-5  ...   \n",
      "Samples                  44k                    44k            44k  ...   \n",
      "Input Length             30k                    30k            30k  ...   \n",
      "\n",
      "Model Name    CNN-2L Var CNN-2L (6k)    NT-1 LoRA r=8 LoRA r=16 LoRA r=32  \\\n",
      "Best Val AUC       0.871      0.8638  0.7322   0.8345    0.8376    0.8453   \n",
      "Model Type          NT-2        NT-2    NT-1     NT-2      NT-2      NT-2   \n",
      "Classifier           CNN         CNN     CNN      CNN       CNN       CNN   \n",
      "Layers                 2           2       2        2         2         2   \n",
      "Embedding            Var         Var     Var      Var       Var       Var   \n",
      "Fine-tuning       Frozen      Frozen  Frozen     LoRA      LoRA      LoRA   \n",
      "LoRA Rank              -           -       -        8        16        32   \n",
      "Learning Rate       3e-5        3e-5    3e-5     3e-5      3e-5      3e-5   \n",
      "Samples              44k         44k     44k       5k        5k        5k   \n",
      "Input Length         12k          6k      6k      12k       12k       12k   \n",
      "\n",
      "Model Name    LR=1e-5 LR=3e-5 LR=5e-5 Full FT  \n",
      "Best Val AUC   0.8793  0.8861  0.8856   0.788  \n",
      "Model Type       NT-2    NT-2    NT-2    NT-2  \n",
      "Classifier        CNN     CNN     CNN     CNN  \n",
      "Layers              2       2       2       2  \n",
      "Embedding         Var     Var     Var     Var  \n",
      "Fine-tuning      LoRA    LoRA    LoRA    Full  \n",
      "LoRA Rank          32      32      32       -  \n",
      "Learning Rate    1e-5    3e-5    5e-5    3e-5  \n",
      "Samples           44k     44k     44k     44k  \n",
      "Input Length      12k     12k     12k     12k  \n",
      "\n",
      "[10 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from model_data\n",
    "table_df = pd.DataFrame(model_data, columns=[\n",
    "    'Model Name', \n",
    "    'Best Val AUC', \n",
    "    'Model Type', \n",
    "    'Classifier', \n",
    "    'Layers', \n",
    "    'Embedding', \n",
    "    'Fine-tuning', \n",
    "    'LoRA Rank', \n",
    "    'Learning Rate', \n",
    "    'Samples',\n",
    "    'Input Length'\n",
    "])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nModel Summary Table:\")\n",
    "print(table_df)\n",
    "\n",
    "# Optionally save to CSV\n",
    "table_df.to_csv(output_dir + 'fig6_model_summary_table.csv', index=False)\n",
    "print(f\"\\nSaved table to {output_dir}fig6_model_summary_table.csv\")\n",
    "\n",
    "# Or transpose it to match the plot table format\n",
    "table_df_transposed = table_df.set_index('Model Name').T\n",
    "print(\"\\nTransposed Table (as shown in plot):\")\n",
    "print(table_df_transposed)\n",
    "\n",
    "# Save transposed version\n",
    "table_df_transposed.to_csv(output_dir + 'fig6_model_summary_table_transposed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec8f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tab_env_2025",
   "language": "python",
   "name": "tab_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
